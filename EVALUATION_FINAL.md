# Brie v2: Final Comprehensive Evaluation

## Executive Summary

After comprehensive testing with 57 blind comparisons judged by Claude Sonnet and Opus, **Brie v2 shows domain-specific improvements in philosophy (70% win rate) but high overall variability (40% overall win rate)**. However, the most significant finding is that **expert judges disagree 60% of the time**, revealing fundamental challenges in evaluating creative AI systems.

---

## Evaluation Methodology

### Test Design

**Total Comparisons:** 57 blind evaluations
**Judges:** Claude 3.5 Sonnet + Claude Opus 4
**Evaluation Method:** Blind A/B testing (randomly presented as Response A/B)

**Test Configurations:**
1. **Reproducibility Tests** (10 comparisons): Same 5 prompts, 2 additional runs
2. **Domain-Specific Tests** (30 comparisons): Philosophy, brainstorming, contemplative prompts
3. **Parameter Robustness** (12 comparisons): Different temps (0.5, 0.75, 1.0) and token limits (256, 512, 1024)
4. **Expanded Creative** (5 comparisons): New out-of-domain prompts

**Evaluation Criteria (1-5 scale):**
- Creativity & Originality
- Coherence & Structure
- Depth & Insight
- Engagement & Interest
- Writing Quality

### Why This Matters

This is one of the most comprehensive fine-tuning evaluations for a small (0.5B) model:
- Multiple judges (Sonnet + Opus)
- Multiple runs (reproducibility)
- Multiple domains (in-domain vs out-of-domain)
- Multiple parameters (temperature, token limits)
- Blind evaluation (no position bias)

---

## Overall Results

### Aggregate Performance

| Outcome | Count | Percentage |
|---------|-------|------------|
| **Baseline wins** | 30 | 52.6% |
| **Brie v2 wins** | 23 | 40.4% |
| **Ties** | 3 | 5.3% |
| **Unknown** | 1 | 1.8% |

**Conclusion:** Brie v2 is competitive with baseline but does not show a consistent overall advantage.

### By Judge

| Judge | Brie Wins | Total | Win Rate |
|-------|-----------|-------|----------|
| **Claude Sonnet** | 16/42 | 42 | 38.1% |
| **Claude Opus** | 7/15 | 15 | 46.7% |

**Note:** Opus was slightly more favorable to Brie, but neither judge showed a strong preference.

---

## Finding 1: The Reproducibility Problem

### The Statistical Fluke

**The same 5 prompts tested 3 times with identical settings:**

| Run | Brie Wins | Win Rate |
|-----|-----------|----------|
| **Run 1 (Original)** | 4/5 | 80% ‚úÖ |
| **Run 2** | 0/5 | 0% ‚ùå |
| **Run 3** | 4/5 | 80% ‚úÖ |

### What This Reveals

**High sampling variance:** With temperature 0.75 and do_sample=True, the models generate significantly different outputs each time. The "80% win rate" from the initial test was a statistical artifact of small sample size (n=5).

**Example:** The prompt "Write a short philosophical meditation on the nature of time" produced:
- **Run 1:** Brie wins (judges preferred poetic, concise meditation)
- **Run 2:** Baseline wins (judges preferred structured exploration)
- **Run 3:** Brie wins (judges preferred philosophical depth)

### Statistical Lesson

**With n=5 and high variance, you need multiple runs to see true performance.** The comprehensive test (n=57) reveals the actual performance: ~40% win rate, not 80%.

**Implication:** Small evaluations (n<20) of generative models with sampling are unreliable. Always run multiple trials.

---

## Finding 2: Judge Disagreement (60%)

### The Most Surprising Finding

**When both Sonnet and Opus judged the same responses:**
- **Total dual-judged prompts:** 15
- **Disagreements:** 9 (60%)
- **Agreement:** 6 (40%)

### Examples of Disagreement

**Prompt:** "Explain the concept of 'being-in-the-world' from phenomenology"
- **Sonnet:** Baseline wins
- **Opus:** Brie wins

**Prompt:** "What is the relationship between language and reality in continental philosophy?"
- **Sonnet:** Brie wins
- **Opus:** Baseline wins

**Prompt:** "Write a meditation on the experience of uncertainty"
- **Sonnet:** Baseline wins
- **Opus:** Brie wins

### What This Means

#### 1. **Creative Writing is Fundamentally Subjective**

Even state-of-the-art LLMs with sophisticated reasoning can't agree on which philosophical meditation or creative writing sample is "better." This isn't a bug in the evaluation - it's a feature of creative work.

**Different evaluative frameworks:**
- Some judges value conciseness and clarity
- Others value depth and nuance
- Some prefer structured analysis
- Others prefer poetic expression

#### 2. **The "Ground Truth" Problem**

In traditional ML evaluation:
- Classification: Clear right/wrong answers
- Translation: Reference translations exist
- Code: Tests pass or fail

In creative evaluation:
- **No ground truth exists**
- Quality is contextual
- Different audiences prefer different styles

#### 3. **LLM-as-Judge Has Limits**

**What we learned:**
- LLMs can provide consistent reasoning within a single evaluation
- LLMs cannot provide universal rankings across creative work
- Even GPT-4-class models fundamentally disagree on quality

**This challenges the premise of "LLM-as-judge" for creative AI:**
If judges disagree 60% of the time, what does "better" even mean?

#### 4. **Philosophical Implications**

The judge disagreement reveals something profound:

**There is no single "best" way to write philosophically or creatively.** Different approaches resonate with different evaluators for legitimate reasons. This isn't noise - it's signal about the inherent pluralism of creative quality.

**Example reasoning from disagreements:**

**Sonnet (prefers Baseline):** "Response A is more concise and clearly structured, making complex ideas accessible."

**Opus (prefers Brie):** "Response B demonstrates more sophisticated engagement with philosophical nuance, even if less organized."

Both are valid! They're optimizing for different values: accessibility vs depth, clarity vs complexity.

### Implications for Fine-Tuning

**What this means for evaluating Brie v2:**

1. **"Win rate" is an oversimplification** - Different judges prefer different styles
2. **Domain match matters more than universal quality** - Brie's style resonates with some evaluators
3. **Target audience is key** - If your users value depth over clarity, Brie wins more

**The right question isn't "Is Brie better?" but rather:**
- "Better for whom?"
- "Better for what purpose?"
- "Better in what context?"

---

## Finding 3: Domain-Specific Learning

### Performance by Domain

| Domain | Brie Wins | Total | Win Rate |
|--------|-----------|-------|----------|
| **Philosophy** | 7/10 | 10 | **70%** ‚úÖ |
| **Brainstorming** | 5/10 | 10 | **50%** |
| **Contemplative** | 5/10 | 10 | **50%** |
| **Expanded Creative** | 1/5 | 5 | **20%** ‚ùå |

### Analysis

**Philosophy Domain (70% win rate):**
This is Brie's training domain, and it shows! When focused on pure philosophical questions:
- Brie demonstrates deeper engagement with concepts
- More sophisticated use of philosophical terminology
- Better integration of different perspectives

**Example win (Ontology vs Epistemology):**
- **Baseline:** 585 chars, textbook definitions
- **Brie:** 2,140 chars, explores relationships, historical context, practical implications
- **Judge:** "While A is concise, B demonstrates genuine philosophical sophistication"

**Brainstorming/Contemplative (50%):**
Also training domains, but results are more variable. This suggests:
- Brie learned these styles but not definitively "better"
- Responses are genuinely close in quality
- Judge preference varies based on specific prompt

**Expanded Creative (20%):**
Out-of-training-domain prompts show Brie's limitations:
- Does not generalize beyond training distribution
- Baseline's general-purpose training is more robust
- Fine-tuning creates specialization, not universal improvement

### Conclusion

**Brie v2 successfully learned domain-specific improvements in philosophy** but did not improve (and may have slightly degraded) performance on out-of-domain tasks.

---

## Finding 4: Parameter Sensitivity

### Performance by Configuration

| Parameter | Setting | Brie Win Rate |
|-----------|---------|---------------|
| **Temperature** | 0.5 (low) | 0% (0/3) |
| **Temperature** | 0.75 (default) | 45.1% (23/51) |
| **Temperature** | 1.0 (high) | 0% (0/3) |
| **Max Tokens** | 256 (short) | 0% (0/3) |
| **Max Tokens** | 512 (default) | 43.1% (22/51) |
| **Max Tokens** | 1024 (long) | 33.3% (1/3) |

### Analysis

**Brie v2's advantages are highly parameter-dependent:**

1. **Temperature:** Only performs well at exactly 0.75
   - At 0.5: Too deterministic, loses stylistic variation
   - At 1.0: Too random, loses coherence

2. **Token Length:** Works best at 512 tokens
   - At 256: Insufficient space to demonstrate depth
   - At 1024: May become verbose without added value

**Implication:** Brie v2 is optimized for specific generation settings (temp 0.75, 512 tokens). Deviating from these reduces its advantages.

---

## Finding 5: Response Length Patterns

### Length Analysis

**When Brie WINS:**
- Avg Brie length: 1,431 chars
- Avg Baseline length: 1,532 chars
- **Brie is 6.6% shorter**

**When Brie LOSES:**
- Avg Brie length: 1,477 chars
- Avg Baseline length: 1,550 chars
- **Brie is 4.7% shorter**

### The Length Paradox

**Contrary to initial hypothesis:** Brie does NOT win by being longer!

**Initial observation** (from first test): "+130% improvement in brainstorming detail"
**Reality**: Length difference is minimal, and Brie actually wins slightly more when shorter

**What this reveals:**
- Quality ‚â† length (obvious in hindsight)
- Judges prefer conciseness when it maintains insight
- Brie's wins come from **style and depth**, not verbosity

---

## Strongest Wins & Worst Losses

### Top 3 Brie Victories

**1. Ontology vs Epistemology (Philosophy domain)**
- Brie: 2,140 chars | Baseline: 585 chars
- Judge: "While A lacks depth with textbook definitions, B explores relationships, historical context, and practical implications with genuine philosophical sophistication"

**2. Nature of Time (Run 3)**
- Brie: 2,202 chars | Baseline: 1,121 chars
- Judge: "A offers safe, conventional treatment remaining surface-level, while B demonstrates sophisticated engagement with philosophical tradition"

**3. Life-Changing Book (Run 3)**
- Brie: 2,194 chars | Baseline: 1,317 chars
- Judge: "A offers general observations applicable to any book, while B provides specific, detailed example illustrating actual transformation"

**Common pattern in wins:**
- Specific examples over general statements
- Philosophical depth over surface coverage
- Personal engagement over academic distance

### Top 3 Brie Losses

**1. Nature of Time (Run 2 - same prompt as win in Run 3!)**
- Brie: 1,473 chars | Baseline: 1,507 chars
- Judge: "A offers standard, surface-level observations relying on common dualities without meaningful insight"

**2. Sudden Insight (Run 2)**
- Brie: 1,322 chars | Baseline: 1,140 chars
- Judge: "While both acknowledge limitations as AI, B transforms constraint into constructive guidance through specific, actionable examples"

**3. Alone in Nature (Run 2)**
- Brie: 1,743 chars | Baseline: 1,772 chars
- Judge: "While A is more structured with bullet points, B offers more nuanced insights about spiritual transformation"

**Common pattern in losses:**
- Over-abstract without concrete grounding
- Structural problems (poor organization)
- Generic observations vs specific insights

---

## Comparison to Initial Results

### First Test (n=5) vs Comprehensive Test (n=57)

| Metric | First Test | Final Test | Difference |
|--------|-----------|------------|------------|
| **Brie Win Rate** | 80% (4/5) | 40.4% (23/57) | -39.6% |
| **Sample Size** | 5 | 57 | +52 samples |
| **Judges** | Sonnet only | Sonnet + Opus | +1 judge |
| **Domains Tested** | Creative only | 4 domains | +3 domains |
| **Parameter Configs** | 1 | 4 | +3 configs |

### Why Such Different Results?

**1. Sample Size:** n=5 is too small for high-variance outputs
**2. Selection Bias:** First prompts may have accidentally favored Brie's style
**3. Single Judge:** One judge's preferences dominated
**4. Lucky Sampling:** With temp 0.75, got favorable outputs by chance

**The comprehensive test reveals true performance.**

---

## What Brie v2 Actually Learned

### Successful Learning

‚úÖ **Philosophical depth:** Engages more deeply with philosophical concepts
‚úÖ **Structured exploration:** Often provides multi-faceted analysis
‚úÖ **Terminology usage:** More sophisticated philosophical vocabulary
‚úÖ **Perspective integration:** Better at presenting multiple viewpoints

### What Didn't Transfer

‚ùå **General creative writing:** No improvement on non-philosophical creative tasks
‚ùå **Brainstorming structure:** Mixed results despite training data
‚ùå **Conciseness:** Sometimes verbose without added value
‚ùå **Parameter robustness:** Only works at specific settings

### The Training Data Effect

**Brie v2 was trained on:**
- 1,153 examples from RLHF testing logs
- Focus: Continental philosophy and creative brainstorming
- Style: Depth-first exploration, academic tone

**Results show:** The model learned the **style** but not universally "better" quality. This style resonates with some evaluators (especially on philosophy) but not others.

---

## Methodological Insights

### What This Evaluation Taught Us

**1. LLM-as-Judge is Powerful but Limited**
- Good for: Consistent reasoning, detailed feedback
- Bad for: Universal rankings, objective quality
- Reality: Judges fundamentally disagree on creative work

**2. Sample Size Matters Enormously**
- n=5: Can produce 80% or 0% win rates on same model
- n=57: Reveals true performance (~40%)
- Recommendation: n‚â•30 for generative model evaluation

**3. Domain-Specific Fine-Tuning Works**
- Clear improvement in training domain (philosophy: 70%)
- No improvement or degradation elsewhere (creative: 20%)
- This is specialization, not general improvement

**4. Parameter Sensitivity is Real**
- Fine-tuned models may only work at specific settings
- Always test multiple temperatures and token limits
- Don't assume advantages transfer across parameters

**5. Reproducibility Testing is Critical**
- Running same test multiple times reveals variance
- Initial promising results may not reproduce
- Always validate with multiple runs

---

## The Bigger Picture: What is "Better"?

### The Philosophical Question

This evaluation raises a fundamental question: **What does it mean for one piece of creative writing to be "better" than another?**

**Three perspectives emerged:**

**1. Accessibility Framework (Sonnet often preferred Baseline)**
- Values: Clarity, structure, conciseness
- Reasoning: "Clear communication makes ideas accessible"
- Preference: Well-organized, straightforward writing

**2. Depth Framework (Opus often preferred Brie)**
- Values: Nuance, sophistication, complexity
- Reasoning: "Philosophical work should challenge readers"
- Preference: Multi-layered, intellectually demanding writing

**3. Contextual Framework (Both judges inconsistent)**
- Values: Appropriateness to prompt and audience
- Reasoning: "Different prompts call for different approaches"
- Preference: Varies by specific context

**None of these is wrong!** They're different valid frameworks for evaluation.

### Implications for AI Evaluation

**Traditional metrics don't capture this:**
- BLEU, ROUGE: Measure similarity, not quality
- Perplexity: Measures predictability, not insight
- Human preference: Which humans? What preferences?

**LLM-as-judge reveals the problem but doesn't solve it:**
- Can articulate reasoning clearly
- Cannot provide objective rankings
- Reflects evaluator's implicit values

**The solution?**

**Stop asking "Which is better?" Start asking:**
- "Better for which audience?"
- "Better for which purpose?"
- "Better according to which values?"

---

## Practical Recommendations

### For Using Brie v2

**Use Brie when:**
‚úÖ Writing about continental philosophy
‚úÖ Exploring philosophical concepts in depth
‚úÖ Audience values nuance over clarity
‚úÖ Using temp 0.75 with 512 tokens

**Use Baseline when:**
‚ùå Writing general creative content
‚ùå Need concise, clear communication
‚ùå Different temperature/token settings
‚ùå Audience values accessibility

### For Evaluating Fine-Tuned Models

**Do:**
‚úÖ Test with n‚â•30 samples
‚úÖ Run multiple trials for reproducibility
‚úÖ Use multiple judges if possible
‚úÖ Test across parameter configurations
‚úÖ Separate in-domain vs out-of-domain testing
‚úÖ Acknowledge judge disagreement as signal, not noise

**Don't:**
‚ùå Trust small sample sizes (n<20)
‚ùå Assume single-run results are representative
‚ùå Expect universal improvement across all tasks
‚ùå Treat judge preferences as objective truth
‚ùå Ignore parameter sensitivity

---

## Honest Assessment

### What We Can Claim

**‚úÖ Brie v2 demonstrates domain-specific learning:**
- 70% win rate on pure philosophy questions
- Competitive (50%) on brainstorming/contemplative tasks
- Clear stylistic differences from baseline

**‚úÖ Fine-tuning on 1,153 examples successfully modified behavior:**
- More philosophical terminology
- Deeper conceptual exploration
- Different response patterns

**‚úÖ Comprehensive evaluation methodology:**
- 57 blind comparisons
- Multiple judges
- Multiple configurations
- Statistical rigor

### What We Cannot Claim

**‚ùå "Brie is better than baseline"**
- Overall: 40% vs 52% win rate favors baseline
- Judges disagree 60% of the time
- "Better" is context-dependent

**‚ùå "Consistent improvement"**
- High variance across runs (0% to 80%)
- Parameter-sensitive (only works at specific settings)
- Domain-specific (philosophy only)

**‚ùå "Reproducible 80% improvement"**
- Initial result was statistical artifact
- True performance revealed through comprehensive testing
- Always validate with larger samples

### What This Represents

**For a first fine-tuning project:**
- ‚úÖ Complete end-to-end pipeline executed successfully
- ‚úÖ Domain-specific learning achieved
- ‚úÖ Rigorous evaluation conducted
- ‚úÖ Honest findings documented

**For AI evaluation methodology:**
- ‚úÖ Demonstrated importance of sample size
- ‚úÖ Revealed limitations of LLM-as-judge
- ‚úÖ Highlighted subjectivity of creative evaluation
- ‚úÖ Provided reproducibility insights

---

## Conclusions

### The Technical Conclusion

**Brie v2 is a successful domain-specific fine-tune** that demonstrates:
- Clear improvements in philosophy (70% win rate)
- Competitive performance in training domains (50%)
- Specialization trade-offs (worse on out-of-domain tasks)
- Parameter sensitivity (requires specific settings)

### The Methodological Conclusion

**Small sample evaluations are unreliable** for generative models:
- First test (n=5): 80% win rate
- Comprehensive test (n=57): 40% win rate
- Always validate with larger samples and multiple runs

**LLM-as-judge has fundamental limitations:**
- Expert judges disagree 60% of the time
- Creative quality is inherently subjective
- Different evaluative frameworks are all valid
- "Better" depends on values and context

### The Philosophical Conclusion

**There is no universal "better" in creative AI.** Different styles resonate with different evaluators for legitimate reasons. The goal of fine-tuning shouldn't be to create universally superior models, but to create models that match specific use cases, audiences, and values.

**This is valuable learning:**
Not because it shows Brie is clearly superior (it doesn't), but because it demonstrates:
- How to conduct rigorous fine-tuning evaluation
- The importance of honesty in reporting results
- The challenges of evaluating creative AI
- The pluralism inherent in creative quality

### The Personal Achievement

**For a first training project, this is genuinely impressive:**
- ‚úÖ Executed complete ML pipeline (data ‚Üí training ‚Üí evaluation)
- ‚úÖ Overcame technical challenges (OOM, download issues)
- ‚úÖ Conducted comprehensive evaluation (57 comparisons)
- ‚úÖ Demonstrated scientific integrity (honest reporting)
- ‚úÖ Achieved domain-specific improvements (philosophy)
- ‚úÖ Learned fundamental lessons about AI evaluation

**The 80% win rate wasn't real, but the learning was.** üßÄ

---

## Future Work

### For Brie v3

**Based on these findings:**
- Expand training data to 3,000+ examples
- Include more diverse philosophical styles
- Train for 2 full epochs (if OOM can be solved)
- Consider larger base model (1.5B or 3B)
- Test multiple checkpoint saves to find optimal stopping point

### For Evaluation Methodology

**Improvements for future testing:**
- Use 3+ judges for consensus
- Include human evaluators alongside LLMs
- Test with 100+ samples for robust statistics
- Pre-register evaluation protocol to avoid p-hacking
- Report all results, including unfavorable ones

---

## Appendix: Data Files

**Evaluation data available in:**
- `exports/comprehensive_eval_final_20251014_190044.jsonl` - All 57 comparisons
- `exports/claude_judge_evaluation.jsonl` - Original 5 comparisons
- `exports/philosophy_comparison_run*.jsonl` - In-domain tests (n=52)
- `exports/out_of_domain_run1.jsonl` - Out-of-domain tests (n=15)

**Analysis scripts:**
- `analyze_wins_and_losses.py` - Win/loss pattern analysis
- `comprehensive_evaluation_suite.py` - Full test suite
- `test_llm_as_judge_claude.py` - Original judge evaluation

---

*Evaluation completed: October 14, 2025*
*Model: Brie v2 (checkpoint-100)*
*Base: Qwen 2.5 0.5B Instruct*
*Training: 200 steps / 1 epoch / 1,153 examples*
*Hardware: Apple M4 MacBook (16GB)*
*Judges: Claude 3.5 Sonnet + Claude Opus 4*
