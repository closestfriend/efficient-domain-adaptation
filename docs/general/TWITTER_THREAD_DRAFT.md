# Twitter Thread: Judge Disagreement in Creative AI Evaluation

---

**Tweet 1 (Hook):**

I fine-tuned a language model on philosophy texts and asked Claude to evaluate it.

Initial result: 80% win rate! üéâ

Then I ran the exact same test again: 0% win rate üò±

Then again: 80% win rate.

Here's what I learned about evaluating creative AI (thread) üßµ

---

**Tweet 2 (The Finding):**

Most surprising finding: When I had TWO expert AI judges (Claude Sonnet + Opus) evaluate the same responses, they disagreed 60% of the time.

Not small disagreements. Completely opposite winners.

Same prompt. Same responses. Opposite conclusions.

---

**Tweet 3 (Example):**

Example:

Prompt: "Explain being-in-the-world from phenomenology"

Sonnet: "Baseline wins - clearer structure, more accessible"
Opus: "Fine-tune wins - more sophisticated philosophical engagement"

They're optimizing for different values! Both are valid.

---

**Tweet 4 (Why It Matters):**

This isn't a bug - it's revealing something fundamental:

In creative work, there IS NO universal "better."

Different audiences want different things:
‚Ä¢ Clarity vs depth
‚Ä¢ Accessibility vs sophistication
‚Ä¢ Structure vs exploration

Quality is contextual, not objective.

---

**Tweet 5 (The Variance Problem):**

The reproducibility problem is wild:

Same 5 prompts, tested 3 times:
‚Ä¢ Run 1: 80% win rate ‚úÖ
‚Ä¢ Run 2: 0% win rate ‚ùå
‚Ä¢ Run 3: 80% win rate ‚úÖ

With temp 0.75 and n=5, results are essentially random.

Small samples + sampling variance = unreliable conclusions.

---

**Tweet 6 (The Truth):**

Comprehensive test with n=57 revealed the truth:

Overall: 40% win rate (not 80%)
Philosophy (training domain): 70% ‚úÖ
General creative: 20% ‚ùå

The initial 80% was a statistical fluke.

This is what rigorous evaluation looks like.

---

**Tweet 7 (Domain Specificity):**

Key insight: Fine-tuning creates SPECIALIZATION, not universal improvement.

My model:
‚úÖ Better at philosophy (70% vs baseline)
‚ùå Worse at general creative (20% vs baseline)

It learned a specific style that resonates with some evaluators, not all.

---

**Tweet 8 (Judge Framework):**

Why judges disagree:

Sonnet often prefers:
‚Ä¢ Clarity, structure, conciseness
‚Ä¢ Practical, accessible writing

Opus often prefers:
‚Ä¢ Depth, nuance, complexity
‚Ä¢ Sophisticated philosophical engagement

Neither is wrong! Different evaluative frameworks.

---

**Tweet 9 (Implications):**

What this means for AI evaluation:

‚ùå Stop treating LLM-as-judge as "objective truth"
‚ùå Stop claiming universal "better"
‚úÖ Use multiple judges, report disagreement
‚úÖ Test with n‚â•30 minimum
‚úÖ Define audience/values first

---

**Tweet 10 (For Researchers):**

If you're doing LLM-as-judge evaluation:

1. Use multiple judges
2. Report disagreement rates (it's signal, not noise!)
3. Test reproducibility (run it 2-3 times)
4. Use large samples (n‚â•30 for creative tasks)
5. Separate in-domain vs out-of-domain

---

**Tweet 11 (For Practitioners):**

Before asking "is this better?", ask:

‚Ä¢ Better for whom?
‚Ä¢ Better for what purpose?
‚Ä¢ Better according to which values?

My model is "better" for philosophy professors who value depth.
It's "worse" for general audiences who want clarity.

Both are true!

---

**Tweet 12 (The Big Lesson):**

The most valuable finding isn't that my model "won" 40% of the time.

It's that expert judges disagreed 60% of the time.

That reveals something fundamental: Creative quality is inherently multi-dimensional and value-laden.

Embrace the pluralism!

---

**Tweet 13 (Honest Science):**

Going from "80% win rate!" to "actually 40%, and judges disagree 60% of the time" was humbling.

But honest negative results are MORE valuable than cherry-picked positives.

This is what rigorous ML evaluation looks like.

Science requires integrity.

---

**Tweet 14 (Personal Context):**

This was my first ML training project:

‚úÖ Complete pipeline (data ‚Üí training ‚Üí eval)
‚úÖ 57 blind comparisons, 2 expert judges
‚úÖ Domain-specific learning achieved (70% on philosophy!)
‚úÖ Scientific integrity maintained
‚úÖ Novel insights about evaluation

Not bad for a first try! üßÄ

---

**Tweet 15 (Takeaway):**

**Key lessons:**

1. Small samples (n<20) are unreliable for creative AI
2. Judge disagreement is signal, not error
3. No universal "better" in creative work
4. Fine-tuning = specialization, not universal improvement
5. Honest reporting > cherry-picked results

Full writeup: [link]

---

**Tweet 16 (Call to Action):**

If you're working on creative AI evaluation, I'd love to hear:

‚Ä¢ Have you seen similar judge disagreement?
‚Ä¢ How do you handle subjectivity in evaluation?
‚Ä¢ What frameworks do you use?

Let's advance the science together!

Code/data: https://github.com/closestfriend/training-off-obsidian

---

**Optional bonus tweet (if people are interested):**

Follow-up: The detailed evaluation doc is 626 lines covering:

‚Ä¢ Statistical analysis of variance
‚Ä¢ Domain-specific performance breakdown
‚Ä¢ Judge disagreement patterns
‚Ä¢ Methodological insights
‚Ä¢ Honest assessment of what worked/didn't

Nerdy ML eval enthusiasts: enjoy! ü§ì

[link to EVALUATION_FINAL.md]

---

**Alt version (more casual/personal):**

I spent 2 weeks training an AI on my philosophy notes.

Initial test: "80% win rate, I'm a genius! üéâ"

Second test: "0% win rate, I'm an idiot üò≠"

Third test: "80% again, I'm confused ü§î"

Turns out: Small sample sizes are liars.

Thread on what I learned about evaluating creative AI üßµ

[then continue with similar thread]
